#!/usr/bin/env python3
"""
QUANTUM ARCHITECT - THE ULTIMATE EXECUTIONER
Beyond analysis. Beyond synthesis. Pure EXECUTION.
Not just understanding‚ÄîCREATING complete, deployable systems.

This is a single-file, self-sufficient automation commander that:
- Ingests a shared knowledge base (repo + knowledge directories)
- Builds a semantic index (TF-IDF, optional embeddings)
- Generates complete blueprints and executable artifacts
- Self-ingests generated artifacts back into the knowledge base
- Supports mutation/autopilot loops to grow knowledge over time

Run:
  python ai_commander_core.py [knowledge_dir]

Interactive commands:
  help
  gather [optional_path]
  search <query>
  execute <vision>
  deploy <blueprint_id>
  blueprints
  export <blueprint_id>
  mutate [seed]
  autopilot <rounds> [seed]
  exit
"""

from __future__ import annotations

import os
import sys
import json
import asyncio
import hashlib
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
from datetime import datetime
import textwrap
import logging
import warnings
import importlib

# Quiet noisy libs
for name in (
    "pdfminer",
    "pdfminer.pdfinterp",
    "pdfminer.converter",
    "pdfminer.layout",
):
    logging.getLogger(name).setLevel(logging.ERROR)
warnings.filterwarnings("ignore", category=UserWarning, module="pdfminer")

# ----- Optional heavy deps with lazy install -----
HAVE_NUMPY = False
HAVE_SKLEARN = False
HAVE_RAPIDFUZZ = False
HAVE_ST = False

try:  # core ML
    import numpy as np
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    HAVE_NUMPY = True
    HAVE_SKLEARN = True
except Exception:
    pass

try:
    from rapidfuzz import fuzz
    HAVE_RAPIDFUZZ = True
except Exception:
    pass

try:
    from sentence_transformers import SentenceTransformer
    HAVE_ST = True
except Exception:
    HAVE_ST = False

# Light-weight PDF fallback
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        from pdfminer.high_level import extract_text
        return extract_text(pdf_path) or ""
    except Exception:
        return ""


# ===== Knowledge Graph =====
@dataclass
class KGDocument:
    file_path: str
    name: str
    category: str
    content: str
    full_content: str
    quality_score: float
    domain_concepts: List[str]


class EnhancedKnowledgeGraph:
    """Dependency-light KG with TF-IDF search, optional embeddings, concept tags, and persistence."""

    def __init__(self, path: str):
        self.knowledge_base_path = Path(path)
        self.knowledge_base_path.mkdir(parents=True, exist_ok=True)

        self.documents: List[KGDocument] = []
        self.relationships: List[Dict[str, Any]] = []
        self.stats: Dict[str, Any] = {}

        # Index state
        self._vectorizer: Optional[TfidfVectorizer] = None
        self._tfidf_matrix = None
        self._corpus: List[str] = []

        self._st_model: Optional[SentenceTransformer] = None
        self._embed_matrix = None

        self._index_file = self.knowledge_base_path / ".qa_index.pkl"
        self._load_index()
        print("üîß Knowledge graph ready ‚Üí", self.knowledge_base_path)

    # ---------- persistence ----------
    def _load_index(self):
        try:
            if self._index_file.exists():
                import pickle
                data = pickle.loads(self._index_file.read_bytes())
                self.documents = [KGDocument(**d) for d in data.get("documents", [])]
                self.stats = data.get("stats", {})
                # We do not load matrices to keep file small; we can rebuild quickly
                print(f"üíæ Loaded KG metadata ({len(self.documents)} docs)")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load KG index: {e}")

    def save_memory(self):
        try:
            import pickle
            blob = {
                "documents": [d.__dict__ for d in self.documents],
                "stats": self.stats,
                "saved_at": datetime.now().isoformat(),
            }
            self._index_file.write_bytes(pickle.dumps(blob))
            print(f"üíæ Saved KG ({len(self.documents)} docs) ‚Üí {self._index_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to save KG: {e}")

    # ---------- ingestion ----------
    def ingest_document(self, file_path: Path, category: str):
        try:
            text = ""
            suf = file_path.suffix.lower()
            if suf in {".txt", ".md", ".rst", ".rxt", ".rtf", ".py", ".js", ".ts", ".tsx", ".json", ".c", ".cpp", ".rs", ".go", ".java", ".cs"}:
                text = file_path.read_text(encoding="utf-8", errors="ignore")
            elif suf == ".docx":
                try:
                    import docx  # python-docx
                    doc = docx.Document(file_path)
                    text = "\n".join(p.text for p in doc.paragraphs)
                except Exception:
                    text = f"Document: {file_path.name}"
            elif suf == ".pdf":
                text = extract_text_from_pdf(str(file_path))

            if not text.strip():
                return

            # Chunk very large files for better indexing and search recall
            CHUNK_THRESHOLD = 50_000
            CHUNK_SIZE = 8_000
            if len(text) > CHUNK_THRESHOLD:
                parts = [text[i : i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]
                for idx, part in enumerate(parts, start=1):
                    name = f"{file_path.name}#part{idx}"
                    doc = KGDocument(
                        file_path=f"{str(file_path)}#part{idx}",
                        name=name,
                        category=category,
                        content=part[:2000],
                        full_content=part,
                        quality_score=0.8,
                        domain_concepts=self._extract_concepts(part),
                    )
                    self.documents.append(doc)
                print(
                    f"üì• Ingested {file_path.name} ‚Üí {category} as {len(parts)} chunks ({len(text)} chars)"
                )
            else:
                doc = KGDocument(
                    file_path=str(file_path),
                    name=file_path.name,
                    category=category,
                    content=text[:2000],
                    full_content=text,
                    quality_score=0.8,
                    domain_concepts=self._extract_concepts(text),
                )
                self.documents.append(doc)
                print(f"üì• Ingested {file_path.name} ‚Üí {category} ({len(text)} chars)")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error ingesting {getattr(file_path, 'name', '<unknown>')}: {e}")

    def ingest_text(self, text: str, category: str = "vision", name: str = "seed.txt"):
        if not text:
            return
        doc = KGDocument(
            file_path=f"<virtual>/{name}",
            name=name,
            category=category,
            content=text[:2000],
            full_content=text,
            quality_score=0.85,
            domain_concepts=self._extract_concepts(text),
        )
        self.documents.append(doc)
        print(f"üå± Seeded virtual document: {name}")

    def ingest_path_recursive(self, root: Path, category: str = "documentation", max_bytes: int = 5_000_000):
        exts = {".txt", ".md", ".rst", ".rxt", ".rtf", ".py", ".js", ".ts", ".tsx", ".json", ".docx", ".pdf"}
        if not root.exists():
            return 0
        count = 0
        for fp in root.rglob("*.*"):
            if fp.suffix.lower() in exts:
                try:
                    if fp.stat().st_size <= max_bytes:
                        self.ingest_document(fp, category=category)
                        count += 1
                except Exception:
                    continue
        return count

    # ---------- indexing & search ----------
    def _ensure_tfidf(self):
        if not HAVE_SKLEARN:
            self._try_install_core()
        if not HAVE_SKLEARN:
            return
        self._corpus = [d.full_content[:10000] for d in self.documents]
        self._vectorizer = TfidfVectorizer(
            lowercase=True,
            ngram_range=(1, 2),
            max_features=20000,
            token_pattern=r"(?u)\b[A-Za-z][A-Za-z0-9_\-]{2,}\b",
        )
        self._tfidf_matrix = self._vectorizer.fit_transform(self._corpus) if self._corpus else None
        if self._tfidf_matrix is not None:
            print(f"üß≠ Built TF-IDF index for {len(self._corpus)} docs")

    def _ensure_embed(self):
        if not HAVE_ST:
            return
        if self._st_model is None:
            try:
                self._st_model = SentenceTransformer("all-MiniLM-L6-v2")
            except Exception:
                self._st_model = None
                return
        texts = [d.full_content[:2000] for d in self.documents]
        if texts:
            self._embed_matrix = self._st_model.encode(texts, show_progress_bar=False)
            print(f"üß† Built embedding index for {len(texts)} docs")

    def build_index(self):
        if not self.documents:
            return
        self._ensure_tfidf()
        self._ensure_embed()

    def semantic_search(self, query: str, limit: int = 10):
        if not self.documents:
            return []
        if self._tfidf_matrix is None and self._embed_matrix is None:
            self.build_index()

        scored: List[Tuple[int, float]] = []

        # Embedding similarity if available
        if self._embed_matrix is not None and self._st_model is not None:
            try:
                qvec = self._st_model.encode([query], show_progress_bar=False)[0]
                import numpy as _np
                sims = (self._embed_matrix @ qvec) / (
                    _np.linalg.norm(self._embed_matrix, axis=1) * (float(_np.linalg.norm(qvec)) + 1e-9)
                )
                for i, s in enumerate(sims.tolist()):
                    scored.append((i, float(s)))
            except Exception:
                pass

        # TF-IDF similarity
        if self._tfidf_matrix is not None and self._vectorizer is not None:
            try:
                qv = self._vectorizer.transform([query])
                sims = cosine_similarity(qv, self._tfidf_matrix)[0]
                for i, s in enumerate(sims.tolist()):
                    scored.append((i, float(s)))
            except Exception:
                pass

        # Fallback: fuzzy ratio on snippet
        if not scored:
            q = query.lower()
            for i, d in enumerate(self.documents):
                snippet = d.content.lower()
                if HAVE_RAPIDFUZZ:
                    score = fuzz.partial_ratio(q, snippet) / 100.0
                else:
                    score = sum(w in snippet for w in q.split()) / max(1, len(q.split()))
                scored.append((i, float(score)))

        # Aggregate by max score per doc
        best: Dict[int, float] = {}
        for i, s in scored:
            best[i] = max(best.get(i, 0.0), s)

        order = sorted(best.items(), key=lambda x: -x[1])[:limit]
        results = [self.documents[i] for (i, _s) in order]
        print(f"‚úÖ Found {len(results)} relevant documents for: '{query}'")
        return results

    # ---------- relationships & centrality ----------
    def build_relationships(self):
        if not self.documents:
            return
        print("üîó Building relationships by concept overlap‚Ä¶")
        concepts_to_docs: Dict[str, List[int]] = defaultdict(list)
        for i, d in enumerate(self.documents):
            for c in d.domain_concepts:
                concepts_to_docs[c].append(i)
        rels: List[Dict[str, Any]] = []
        for c, idxs in concepts_to_docs.items():
            if len(idxs) < 2:
                continue
            for a in range(len(idxs) - 1):
                for b in range(a + 1, len(idxs)):
                    rels.append({
                        "source": self.documents[idxs[a]].file_path,
                        "target": self.documents[idxs[b]].file_path,
                        "type": "shares_concepts",
                        "concept": c,
                    })
        self.relationships = rels

    def calculate_centrality(self):
        degrees: Dict[str, int] = defaultdict(int)
        for r in self.relationships:
            degrees[r["source"]] += 1
            degrees[r["target"]] += 1
        for d in self.documents:
            deg = degrees.get(d.file_path, 0)
            d.quality_score = 0.7 + 0.05 * min(6, deg)

    # ---------- helpers ----------
    def _extract_concepts(self, text: str) -> List[str]:
        words = re.findall(r"\b[A-Za-z][A-Za-z0-9\-]{2,}\b", text)
        caps = [w for w in words if w[0].isupper() and w.lower() != w]
        hot = {
            "blockchain",
            "ai",
            "machine",
            "learning",
            "web3",
            "nft",
            "token",
            "smart",
            "contract",
            "react",
            "nextjs",
            "fastapi",
            "docker",
            "kubernetes",
            "microservices",
            "architecture",
            "postgres",
            "redis",
        }
        tags = set([w.title() for w in caps if 3 < len(w) < 28] + [w.title() for w in words if w.lower() in hot])
        return sorted(list(tags))[:12]

    def _try_install_core(self):
        global HAVE_NUMPY, HAVE_SKLEARN, HAVE_RAPIDFUZZ
        try:
            print("‚öôÔ∏è  Installing core deps (numpy, scikit-learn, rapidfuzz)‚Ä¶")
            os.system("pip install --quiet numpy scikit-learn rapidfuzz pdfminer.six python-docx jinja2")
            import numpy as _np  # noqa: F401
            from sklearn.feature_extraction.text import TfidfVectorizer as _TV  # noqa: F401
            from rapidfuzz import fuzz as _f  # noqa: F401
            HAVE_NUMPY = True
            HAVE_SKLEARN = True
            HAVE_RAPIDFUZZ = True
        except Exception:
            pass


# ===== Document/NLP utilities =====
class DocumentDigester:
    """Lightweight summarization and repository description utilities.
    Uses deterministic heuristics (no heavy models by default) for speed/repeatability.
    """

    CODE_EXTS = {".py", ".ts", ".tsx", ".js", ".jsx", ".go", ".rs", ".java", ".c", ".cpp"}

    def __init__(self, kg: EnhancedKnowledgeGraph):
        self.kg = kg

    def summarize(self, text: str, title: str, max_len: int = 10) -> str:
        # Reuse commander's summarizer if available later; keep self-contained here.
        sentences = re.split(r"(?<=[.!?])\s+", text.strip())
        tokens = [w.lower() for w in re.findall(r"[A-Za-z][A-Za-z0-9_\-]{2,}", text)]
        stop = {"the", "and", "for", "with", "this", "that", "have", "from", "they", "will", "your", "you"}
        freq = defaultdict(int)
        for t in tokens:
            if t not in stop:
                freq[t] += 1
        scored = []
        for s in sentences:
            score = 0
            for t in re.findall(r"[A-Za-z][A-Za-z0-9_\-]{2,}", s.lower()):
                score += freq.get(t, 0)
            scored.append((score, s))
        scored.sort(key=lambda x: -x[0])
        top = [s for _, s in scored[:max_len] if s.strip()]
        outline = "\n".join(f"- {s.strip()}" for s in top)
        return f"# {title}\n{outline}"

    def summarize_code(self, text: str, title: str) -> str:
        lines = text.splitlines()
        bullets: List[str] = []
        # Python
        for m in re.finditer(r"^\s*class\s+([A-Za-z_][A-Za-z0-9_]*)", text, flags=re.M):
            bullets.append(f"class {m.group(1)}")
        for m in re.finditer(r"^\s*def\s+([A-Za-z_][A-Za-z0-9_]*)\(", text, flags=re.M):
            bullets.append(f"def {m.group(1)}(...)" )
        # JS/TS
        for m in re.finditer(r"^\s*export\s+(?:default\s+)?class\s+([A-Za-z_][A-Za-z0-9_]*)", text, flags=re.M):
            bullets.append(f"class {m.group(1)}")
        for m in re.finditer(r"^\s*export\s+function\s+([A-Za-z_][A-Za-z0-9_]*)\(", text, flags=re.M):
            bullets.append(f"function {m.group(1)}(...)")
        # Components
        for m in re.finditer(r"^\s*export\s+const\s+([A-Za-z_][A-Za-z0-9_]*)\s*=\s*\(", text, flags=re.M):
            bullets.append(f"component {m.group(1)}")
        bullets = list(dict.fromkeys(bullets))
        header = f"# {title}\n" + ("\n".join(f"- {b}" for b in bullets[:30]) if bullets else "- (no top-level symbols found)")
        # Add brief extractive summary tail
        head = "\n".join(lines[:80])
        summary_tail = self.summarize(head, title + " (extract)", max_len=6)
        return header + "\n\n" + summary_tail

    def describe_repo(self, root: Path) -> str:
        lang_counts = defaultdict(int)
        files = list(root.rglob("*.*")) if root.exists() else []
        for fp in files:
            lang_counts[fp.suffix.lower()] += 1
        total = sum(lang_counts.values())
        top_langs = sorted(lang_counts.items(), key=lambda x: -x[1])[:10]
        frameworks = []
        # Simple detection
        if (root / "package.json").exists():
            frameworks.append("Node.js")
        if (root / "apps" / "web").exists():
            frameworks.append("Next.js")
        if list(root.rglob("*fastapi*")) or list(root.rglob("*api/routes/*.py")):
            frameworks.append("FastAPI")
        if list(root.rglob("Dockerfile")):
            frameworks.append("Docker")
        lines = [
            f"Files: {total}",
            "Languages: " + ", ".join(f"{k}:{v}" for k, v in top_langs),
            "Frameworks: " + (", ".join(sorted(set(frameworks))) or "(none detected)"),
        ]
        return "# Repository Architecture\n" + "\n".join(f"- {l}" for l in lines)


# ===== NLP Interface =====
class NLPEngine:
    """Minimal NLP interface: intent parsing + RAG-backed summarization and creators.
    Deterministic and dependency-light; integrate heavier models externally if desired.
    """

    def __init__(self, kg: EnhancedKnowledgeGraph, digester: DocumentDigester, architect: 'QuantumArchitect'):
        self.kg = kg
        self.digester = digester
        self.architect = architect

    def parse_intent(self, text: str) -> Dict[str, Any]:
        t = text.strip()
        low = t.lower()
        if low.startswith("locate "):
            return {"intent": "locate", "query": t[7:].strip()}
        if low.startswith("understand "):
            return {"intent": "understand", "query": t[11:].strip()}
        if low.startswith("describe "):
            return {"intent": "describe", "target": t[9:].strip()}
        if low.startswith("explain "):
            return {"intent": "explain", "target": t[8:].strip()}
        if low.startswith("create "):
            return {"intent": "create", "spec": t[7:].strip()}
        return {"intent": "qa", "query": t}

    def handle(self, text: str) -> Tuple[str, List[ExecutableArtifact]]:
        intent = self.parse_intent(text)
        it = intent["intent"]
        if it == "locate":
            q = intent["query"]
            hits = self.kg.semantic_search(q, limit=10)
            lines = [f"- {Path(d.file_path).name} | {d.category} | ‚≠ê {d.quality_score:.2f}" for d in hits]
            return ("\n".join(lines) or "(no results)", [])
        if it in {"understand", "explain"}:
            target = intent.get("query") or intent.get("target") or ""
            p = Path(target)
            if target and p.exists():
                if p.is_file():
                    text = p.read_text(encoding="utf-8", errors="ignore")
                    if p.suffix.lower() in DocumentDigester.CODE_EXTS:
                        s = self.digester.summarize_code(text, p.name)
                    else:
                        s = self.digester.summarize(text, p.name)
                    return (s, [])
                else:
                    s = self.digester.describe_repo(p)
                    return (s, [])
            # fallback to KG
            docs = self.kg.semantic_search(target or "overview", limit=6)
            blob = []
            for d in docs:
                if any(ext in d.name.lower() for ext in [".py", ".ts", ".js", ".go", ".rs", ".java"]):
                    blob.append(self.digester.summarize_code(d.full_content, d.name))
                else:
                    blob.append(self.digester.summarize(d.full_content, d.name))
            return ("\n\n".join(blob[:6]) or "(no context)", [])
        if it == "describe":
            target = intent.get("target", ".")
            return (self.digester.describe_repo(Path(target)), [])
        if it == "create":
            spec = intent.get("spec", "")
            artifacts: List[ExecutableArtifact] = []
            # very simple pattern-based creation
            m = re.search(r"react component\s+([A-Za-z_][A-Za-z0-9_]*)", spec, flags=re.I)
            if m:
                name = m.group(1)
                props: Dict[str, str] = {}
                pm = re.search(r"props\s*:\s*([^;]+)", spec, flags=re.I)
                if pm:
                    for pair in pm.group(1).split(","):
                        if ":" in pair:
                            k, v = pair.split(":", 1)
                            props[k.strip()] = v.strip()
                art = self.architect.code_gen.generate_react_component(name=name, props=props or {"title": "string"}, features=["data-fetching"])
                artifacts.append(art)
                return (f"Created React component {name}", artifacts)
            m2 = re.search(r"fastapi service\s+([A-Za-z_][A-Za-z0-9_]*)", spec, flags=re.I)
            if m2:
                name = m2.group(1)
                ep = {"prefix": "api/v1", "tag": name.lower(), "path": f"/{name.lower()}", "function": f"create_{name.lower()}"}
                art = self.architect.code_gen.generate_fastapi_service(name=name, endpoint=ep)
                artifacts.append(art)
                return (f"Created FastAPI service {name}", artifacts)
            # fallback: output instruction not understood
            return ("(create spec not recognized)", [])
        # QA: list top relevant docs titles
        hits = self.kg.semantic_search(intent.get("query", ""), limit=8)
        return ("\n".join(f"- {Path(d.file_path).name}" for d in hits) or "(no results)", [])

# ===== Artifacts & Blueprints =====
@dataclass
class ExecutableArtifact:
    id: str
    name: str
    artifact_type: str  # code | config | architecture | design | deployment
    language: str
    content: str
    file_path: Optional[str] = None
    dependencies: List[str] = field(default_factory=list)
    deployment_ready: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class QuantumBlueprint:
    id: str
    title: str
    description: str
    architecture: Dict[str, Any]
    artifacts: List[ExecutableArtifact] = field(default_factory=list)
    deployment_plan: Dict[str, Any] = field(default_factory=dict)
    tech_stack: Dict[str, List[str]] = field(default_factory=dict)
    estimated_timeline: Dict[str, str] = field(default_factory=dict)
    cost_estimate: Dict[str, Any] = field(default_factory=dict)
    deployment_commands: List[str] = field(default_factory=list)
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())


# ===== Quantum Code Generator (shortened templates kept practical) =====
class QuantumCodeGenerator:
    TEMPLATES = {
        "react_app": textwrap.dedent(
            """
            // {component_name}.tsx - Auto-generated by Quantum Architect
            import React, {{ useState, useEffect }} from 'react';

            interface {component_name}Props {{
              {props}
            }}

            export const {component_name}: React.FC<{component_name}Props> = ({{ {prop_names} }}) => {{
              {state_declarations}

              useEffect(() => {{
                {init_logic}
              }}, []);

              {functions}

              return (
                <div className="container mx-auto p-4">
                  {jsx_content}
                </div>
              );
            }};

            export default {component_name};
            """
        ),
        "fastapi_endpoint": textwrap.dedent(
            """
            # {endpoint_name}.py - Auto-generated by Quantum Architect
            from fastapi import APIRouter, HTTPException, status
            from pydantic import BaseModel, Field
            from typing import Dict, Any
            from datetime import datetime

            router = APIRouter(prefix="/{prefix}", tags=["{tag}"])

            class {model_name}Request(BaseModel):
                {request_fields}

                class Config:
                    json_schema_extra = {{"example": {example_data}}}

            class {model_name}Response(BaseModel):
                {response_fields}
                created_at: datetime = Field(default_factory=datetime.now)

            @router.post("/{endpoint}", response_model={model_name}Response)
            async def {function_name}(request: {model_name}Request):
                try:
                    {business_logic}
                    return {model_name}Response({return_data})
                except Exception as e:
                    raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))
            """
        ),
        "dockerfile": textwrap.dedent(
            """
            # Dockerfile - Auto-generated by Quantum Architect
            FROM {base_image}
            WORKDIR /app
            COPY {requirements_file} .
            RUN {install_command}
            COPY . .
            ENV {env_vars}
            EXPOSE {port}
            CMD [{start_command}]
            """
        ),
        "kubernetes_deployment": textwrap.dedent(
            """
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: {app_name}
              namespace: {namespace}
            spec:
              replicas: {replicas}
              selector:
                matchLabels:
                  app: {app_name}
              template:
                metadata:
                  labels:
                    app: {app_name}
                spec:
                  containers:
                    - name: {container_name}
                      image: {image}
                      ports:
                        - containerPort: {port}
            ---
            apiVersion: v1
            kind: Service
            metadata:
              name: {app_name}-svc
              namespace: {namespace}
            spec:
              selector:
                app: {app_name}
              ports:
                - protocol: TCP
                  port: 80
                  targetPort: {port}
              type: {service_type}
            """
        ),
        "github_actions": textwrap.dedent(
            """
            name: Deploy {app_name}
            on:
              push:
                branches: [ {branch} ]
            jobs:
              test:
                runs-on: ubuntu-latest
                steps:
                  - uses: actions/checkout@v3
                  - name: Setup {runtime}
                    uses: actions/setup-{runtime}@v3
                    with:
                      {runtime}-version: '{version}'
                  - name: Install
                    run: {install_cmd}
                  - name: Test
                    run: {test_cmd}
              build:
                needs: test
                runs-on: ubuntu-latest
                steps:
                  - uses: actions/checkout@v3
                  - name: Build
                    run: docker build -t {image_name} .
              deploy:
                needs: build
                runs-on: ubuntu-latest
                steps:
                  - name: Deploy
                    run: |
                      {deploy_commands}
            """
        ),
    }

    @staticmethod
    def generate_react_component(name: str, props: Dict[str, str], features: List[str]) -> ExecutableArtifact:
        props_str = "\n  ".join([f"{k}: {v};" for k, v in props.items()])
        prop_names = ", ".join(props.keys())
        state_declarations: List[str] = []
        functions: List[str] = []

        if "data-fetching" in features:
            state_declarations += [
                "const [data, setData] = useState<any[]>([]);",
                "const [loading, setLoading] = useState(false);",
            ]
            functions.append(
                textwrap.dedent(
                    """
                    const fetchData = async () => {
                      setLoading(true);
                      try {
                        const response = await fetch('/api/data');
                        const json = await response.json();
                        setData(json);
                      } catch (error) {
                        console.error('Error fetching data:', error);
                      } finally {
                        setLoading(false);
                      }
                    };
                    """
                ).strip()
            )

        jsx_content = textwrap.dedent(
            """
            <div className="header"><h1>{title}</h1></div>
            <div className="content">
              {loading ? (
                <div className="loading">Loading...</div>
              ) : (
                <div className="data-container">
                  {data.map((item, index) => (
                    <div key={index} className="data-item">{JSON.stringify(item)}</div>
                  ))}
                </div>
              )}
            </div>
            """
        ).strip()

        code = QuantumCodeGenerator.TEMPLATES["react_app"].format(
            component_name=name,
            props=props_str,
            prop_names=prop_names,
            state_declarations="\n  ".join(state_declarations),
            init_logic="fetchData();" if "data-fetching" in features else "",
            functions="\n\n".join(functions),
            jsx_content=jsx_content,
        )

        return ExecutableArtifact(
            id=hashlib.md5(name.encode()).hexdigest(),
            name=f"{name}.tsx",
            artifact_type="code",
            language="typescript",
            content=code,
            file_path=f"src/components/{name}.tsx",
            dependencies=["react", "typescript"],
            deployment_ready=True,
            metadata={"framework": "React", "features": features},
        )

    @staticmethod
    def generate_fastapi_service(name: str, endpoint: Dict[str, Any]) -> ExecutableArtifact:
        request_fields = "\n                ".join(
            [f"{f['name']}: {f['type']}" for f in endpoint.get("request_fields", [])]
        ) or "data: str"
        response_fields = "\n                ".join(
            [f"{f['name']}: {f['type']}" for f in endpoint.get("response_fields", [])]
        ) or "result: str"

        code = QuantumCodeGenerator.TEMPLATES["fastapi_endpoint"].format(
            endpoint_name=name.lower(),
            prefix=endpoint.get("prefix", "api"),
            tag=endpoint.get("tag", "default"),
            model_name=name,
            request_fields=request_fields,
            response_fields=response_fields,
            example_data='{"data": "example"}',
            endpoint=endpoint.get("path", f"/{name.lower()}"),
            function_name=endpoint.get("function", name.lower()),
            business_logic="result = {'ok': True}",
            return_data="result=result",
        )

        return ExecutableArtifact(
            id=hashlib.md5(name.encode()).hexdigest(),
            name=f"{name.lower()}_routes.py",
            artifact_type="code",
            language="python",
            content=code,
            file_path=f"api/routes/{name.lower()}_routes.py",
            dependencies=["fastapi", "pydantic"],
            deployment_ready=True,
            metadata={"framework": "FastAPI", "endpoint": endpoint.get("path")},
        )

    @staticmethod
    def generate_infrastructure(app_name: str, services: List[str], provider: str = "gcp") -> List[ExecutableArtifact]:
        artifacts: List[ExecutableArtifact] = []

        docker_config = {
            "base_image": "node:18-alpine" if "frontend-web" in services else "python:3.11-slim",
            "requirements_file": "package.json" if "frontend-web" in services else "requirements.txt",
            "install_command": "npm install" if "frontend-web" in services else "pip install -r requirements.txt",
            "env_vars": "NODE_ENV=production" if "frontend-web" in services else "PYTHONUNBUFFERED=1",
            "port": "3000" if "frontend-web" in services else "8000",
            "start_command": '"npm", "start"' if "frontend-web" in services else '"uvicorn", "main:app", "--host", "0.0.0.0"',
        }
        dockerfile = QuantumCodeGenerator.TEMPLATES["dockerfile"].format(**docker_config)
        artifacts.append(
            ExecutableArtifact(
                id=hashlib.md5(b"dockerfile").hexdigest(),
                name="Dockerfile",
                artifact_type="config",
                language="dockerfile",
                content=dockerfile,
                file_path="Dockerfile",
                deployment_ready=True,
            )
        )

        k8s_config = {
            "app_name": app_name,
            "namespace": "production",
            "replicas": "3",
            "container_name": f"{app_name}-container",
            "image": f"{provider}.io/{app_name}:latest",
            "port": "8000",
            "service_type": "LoadBalancer",
        }
        k8s_deployment = QuantumCodeGenerator.TEMPLATES["kubernetes_deployment"].format(**k8s_config)
        artifacts.append(
            ExecutableArtifact(
                id=hashlib.md5(b"k8s").hexdigest(),
                name="kubernetes-deployment.yaml",
                artifact_type="config",
                language="yaml",
                content=k8s_deployment,
                file_path="kubernetes/deployment.yaml",
                deployment_ready=True,
            )
        )

        ci_config = {
            "app_name": app_name,
            "branch": "main",
            "runtime": "node" if "frontend-web" in services else "python",
            "version": "18" if "frontend-web" in services else "3.11",
            "install_cmd": "npm ci" if "frontend-web" in services else "pip install -r requirements.txt",
            "test_cmd": "npm test" if "frontend-web" in services else "pytest",
            "image_name": f"{provider}.io/{app_name}:${{ github.sha }}",
            "deploy_commands": "echo 'deploying...'",
        }
        ci_cd = QuantumCodeGenerator.TEMPLATES["github_actions"].format(**ci_config)
        artifacts.append(
            ExecutableArtifact(
                id=hashlib.md5(b"ci").hexdigest(),
                name="deploy.yml",
                artifact_type="config",
                language="yaml",
                content=ci_cd,
                file_path=".github/workflows/deploy.yml",
                deployment_ready=True,
            )
        )

        return artifacts


# ===== Quantum Architect =====
class QuantumArchitect:
    def __init__(self, knowledge_graph: EnhancedKnowledgeGraph):
        self.kg = knowledge_graph
        self.code_gen = QuantumCodeGenerator()
        self.blueprints: Dict[str, QuantumBlueprint] = {}
        self.run_log: List[str] = []
        print("\n" + "‚öîÔ∏è" * 35)
        print("üî± QUANTUM ARCHITECT INITIALIZED")
        print("   THE EXECUTIONER - Not just planning, BUILDING")
        print("‚öîÔ∏è" * 35)

    async def execute_vision(self, vision: str, deployment_target: str = "cloud") -> Optional[QuantumBlueprint]:
        print(f"\n{'=' * 70}")
        print(f"‚öîÔ∏è  EXECUTING VISION: {vision}")
        print(f"üéØ Deployment Target: {deployment_target}")
        print(f"{'=' * 70}\n")

        print("üß† Phase 1: Intelligence Gathering‚Ä¶")
        knowledge = self.kg.semantic_search(vision, limit=25)
        if not knowledge:
            print("‚ùå Insufficient knowledge base")
            return None

        print("üèóÔ∏è  Phase 2: Architecture Design‚Ä¶")
        architecture = await self._design_architecture(vision, knowledge)

        print("üíª Phase 3: Code Generation‚Ä¶")
        artifacts = await self._generate_artifacts(architecture)

        print("üöÄ Phase 4: Infrastructure Provisioning‚Ä¶")
        infra_artifacts = await self._generate_infrastructure(architecture, deployment_target)
        artifacts.extend(infra_artifacts)

        print("üìã Phase 5: Deployment Strategy‚Ä¶")
        deployment_plan = await self._create_deployment_plan(architecture, artifacts, deployment_target)

        print("üí∞ Phase 6: Cost Analysis‚Ä¶")
        cost_estimate = self._estimate_costs(architecture, deployment_target)

        blueprint_id = hashlib.md5(f"{vision}_{datetime.now().isoformat()}".encode()).hexdigest()
        blueprint = QuantumBlueprint(
            id=blueprint_id,
            title=vision,
            description=f"Complete executable system for: {vision}",
            architecture=architecture,
            artifacts=artifacts,
            deployment_plan=deployment_plan,
            tech_stack=architecture.get("tech_stack", {}),
            estimated_timeline=self._generate_timeline(architecture),
            cost_estimate=cost_estimate,
            deployment_commands=deployment_plan.get("commands", []),
        )
        self.blueprints[blueprint_id] = blueprint

        # Self-ingest blueprint summary to grow knowledge
        self._self_ingest_blueprint(blueprint)

        print(f"\n{'=' * 70}")
        print("‚úÖ EXECUTION COMPLETE")
        print(f"   üì¶ Artifacts Generated: {len(artifacts)}")
        print(
            f"   üíæ Total LOC: {sum(len(a.content.splitlines()) for a in artifacts)}"
        )
        print(
            f"   üí∞ Estimated Monthly Cost: ${cost_estimate.get('monthly_total', 0)}"
        )
        print(f"   ‚è±Ô∏è  Timeline: {self._get_timeline_summary(blueprint.estimated_timeline)}")
        print(f"{'=' * 70}\n")

        return blueprint

    async def _design_architecture(self, vision: str, knowledge_docs: List[KGDocument]) -> Dict[str, Any]:
        is_web_app = any(t in vision.lower() for t in ["web", "app", "platform", "dashboard"])
        is_api = any(t in vision.lower() for t in ["api", "service", "backend"])
        is_blockchain = any(t in vision.lower() for t in ["blockchain", "web3", "nft", "token"])
        is_ai = any(t in vision.lower() for t in ["ai", "ml", "intelligence", "learning"])()

        # Correction for callable typo above
        is_ai = any(t in vision.lower() for t in ["ai", "ml", "intelligence", "learning"])  # noqa: E501

        tech_mentions: Dict[str, int] = defaultdict(int)
        for d in knowledge_docs:
            l = d.content.lower()
            tech_mentions["react"] += ("react" in l) or ("next" in l)
            tech_mentions["fastapi"] += ("fastapi" in l) or ("python" in l)
            tech_mentions["ethereum"] += ("ethereum" in l) or ("solidity" in l)
            tech_mentions["postgresql"] += ("postgres" in l)

        architecture: Dict[str, Any] = {
            "vision": vision,
            "architecture_pattern": "Microservices + Event-Driven",
            "deployment_model": "Cloud-Native Kubernetes",
            "layers": {},
            "tech_stack": {},
            "services": [],
            "integrations": [],
            "scalability": {},
            "security": [],
        }

        if is_web_app:
            architecture["layers"]["frontend"] = {
                "framework": "Next.js 14",
                "language": "TypeScript",
                "styling": "Tailwind",
                "state": "Zustand/TanStack",
            }
            architecture["tech_stack"]["frontend"] = ["Next.js", "React", "TypeScript", "Tailwind"]
            architecture["services"].append("frontend-web")

        if is_api or is_web_app:
            architecture["layers"]["backend"] = {
                "framework": "FastAPI",
                "features": ["Async", "Pydantic", "Auto Docs"],
            }
            architecture["tech_stack"]["backend"] = ["FastAPI", "Python", "Pydantic", "SQLAlchemy"]
            architecture["services"] += ["auth-service", "api-gateway", "core-service"]

        architecture["layers"]["database"] = {
            "primary": "PostgreSQL 15",
            "cache": "Redis 7",
        }
        architecture["tech_stack"]["database"] = ["PostgreSQL", "Redis"]

        if is_blockchain:
            architecture["layers"]["blockchain"] = {
                "network": "Ethereum",
                "contracts": "Solidity 0.8.20",
            }
            architecture["tech_stack"]["blockchain"] = ["Ethereum", "Solidity"]
            architecture["services"].append("blockchain-service")

        if is_ai:
            architecture["layers"]["ai"] = {
                "framework": "LangChain + HF",
                "models": "GPT-4 API",
            }
            architecture["tech_stack"]["ai"] = ["LangChain", "OpenAI"]
            architecture["services"].append("ai-service")

        architecture["layers"]["infrastructure"] = {
            "container": "Docker",
            "orchestration": "Kubernetes",
            "ci_cd": "GitHub Actions",
        }

        architecture["security"] = [
            "Zero-Trust",
            "mTLS",
            "JWT Rotation",
            "Rate limiting",
        ]

        return architecture

    async def _generate_artifacts(self, architecture: Dict[str, Any]) -> List[ExecutableArtifact]:
        artifacts: List[ExecutableArtifact] = []
        if "frontend" in architecture["layers"]:
            artifacts.append(
                self.code_gen.generate_react_component(
                    name="Dashboard",
                    props={"title": "string", "user": "any"},
                    features=["data-fetching"],
                )
            )

        if "backend" in architecture["layers"]:
            artifacts.append(
                self.code_gen.generate_fastapi_service(
                    name="User",
                    endpoint={
                        "prefix": "api/v1",
                        "tag": "users",
                        "path": "/users",
                        "function": "create_user",
                        "request_fields": [
                            {"name": "email", "type": "str"},
                            {"name": "name", "type": "str"},
                        ],
                        "response_fields": [
                            {"name": "id", "type": "str"},
                            {"name": "email", "type": "str"},
                        ],
                    },
                )
            )

        # Config files
        compose = textwrap.dedent(
            """
            version: '3.8'
            services:
              backend:
                build: ./backend
                ports: ["8000:8000"]
              frontend:
                build: ./frontend
                ports: ["3000:3000"]
            """
        ).strip()
        artifacts.append(
            ExecutableArtifact(
                id=hashlib.md5(b"compose").hexdigest(),
                name="docker-compose.yml",
                artifact_type="config",
                language="yaml",
                content=compose,
                file_path="docker-compose.yml",
                deployment_ready=True,
            )
        )
        return artifacts

    async def _generate_infrastructure(self, architecture: Dict[str, Any], target: str) -> List[ExecutableArtifact]:
        services = list(architecture.get("services", []))
        return self.code_gen.generate_infrastructure(
            app_name="quantum-app", services=services, provider="gcr"
        )

    async def _create_deployment_plan(
        self, architecture: Dict[str, Any], artifacts: List[ExecutableArtifact], target: str
    ) -> Dict[str, Any]:
        commands = [
            "# Local Dev",
            "docker-compose up -d",
            "# Frontend http://localhost:3000",
            "# Backend http://localhost:8000",
            "",
            "# Kubernetes",
            "kubectl apply -f kubernetes/deployment.yaml",
        ]
        return {
            "target": target,
            "commands": commands,
            "prerequisites": [
                "Docker",
                "kubectl",
                "Git",
            ],
            "estimated_time": "30-60 minutes",
        }

    def _estimate_costs(self, architecture: Dict[str, Any], target: str) -> Dict[str, Any]:
        num_services = len(architecture.get("services", []))
        costs = {
            "compute": num_services * 50,
            "database": 100,
            "storage": 25,
            "networking": 50,
            "services": 50,
        }
        costs["monthly_total"] = sum(costs.values())
        costs["annual_estimate"] = costs["monthly_total"] * 12
        costs["breakdown"] = {
            "Kubernetes": 150,
            "PostgreSQL": 100,
            "Redis": 30,
            "CDN": 20,
        }
        return costs

    def _generate_timeline(self, architecture: Dict[str, Any]) -> Dict[str, str]:
        base_weeks = 9
        return {
            "Phase 1": "Weeks 1-2",
            "Phase 2": "Weeks 3-5",
            "Phase 3": "Weeks 6-7",
            "Phase 4": "Weeks 8-9",
            "Total Duration": "10 weeks",
        }

    def _get_timeline_summary(self, timeline: Dict[str, str]) -> str:
        return timeline.get("Total Duration", "N/A")

    def _self_ingest_blueprint(self, blueprint: QuantumBlueprint):
        summary = {
            "id": blueprint.id,
            "title": blueprint.title,
            "tech_stack": blueprint.tech_stack,
            "services": blueprint.architecture.get("services", []),
            "artifact_paths": [a.file_path for a in blueprint.artifacts],
        }
        self.kg.ingest_text(
            text=json.dumps(summary, indent=2),
            category="blueprint",
            name=f"blueprint_{blueprint.id}.json",
        )
        # Also ingest each artifact's content virtually (lightweight, ensures searchability)
        for a in blueprint.artifacts:
            # For large artifacts, chunk to improve retrieval quality
            content = a.content
            CHUNK_SIZE = 6_000
            if len(content) > CHUNK_SIZE:
                chunks = [content[i : i + CHUNK_SIZE] for i in range(0, len(content), CHUNK_SIZE)]
                for idx, chunk in enumerate(chunks, start=1):
                    self.kg.ingest_text(
                        text=chunk,
                        category="artifact",
                        name=f"{(a.file_path or a.name)}#part{idx}",
                    )
            else:
                self.kg.ingest_text(
                    text=content,
                    category="artifact",
                    name=a.file_path or a.name,
                )
        self.kg.build_index()
        self.kg.build_relationships()
        self.kg.calculate_centrality()
        self.kg.save_memory()

    async def deploy_blueprint(self, blueprint_id: str, environment: str = "staging") -> Dict[str, Any]:
        blueprint = self.blueprints.get(blueprint_id)
        if not blueprint:
            return {"error": "Blueprint not found"}

        print(f"\n{'=' * 70}")
        print(f"üöÄ DEPLOYING BLUEPRINT: {blueprint.title}")
        print(f"üéØ Environment: {environment}")
        print(f"{'=' * 70}\n")

        output_dir = Path(f"output/{blueprint_id}_{environment}")
        output_dir.mkdir(parents=True, exist_ok=True)

        deployment_log: List[str] = []
        for artifact in blueprint.artifacts:
            file_path = output_dir / (artifact.file_path or artifact.name)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            file_path.write_text(artifact.content)
            deployment_log.append(f"‚úÖ Created: {artifact.file_path}")
            print(f"   ‚úÖ {artifact.file_path}")

            # Self-ingest persisted artifact
            try:
                self.kg.ingest_document(file_path, category="artifact")
            except Exception:
                pass

        readme = self._generate_deployment_readme(blueprint)
        (output_dir / "DEPLOYMENT.md").write_text(readme)
        deployment_log.append("‚úÖ Generated: DEPLOYMENT.md")

        deploy_script = self._generate_deploy_script(blueprint)
        script_path = output_dir / "deploy.sh"
        script_path.write_text(deploy_script)
        script_path.chmod(0o755)
        deployment_log.append("‚úÖ Created: deploy.sh")

        # Update KG with new files
        self.kg.build_index()
        self.kg.build_relationships()
        self.kg.calculate_centrality()
        self.kg.save_memory()

        print(f"\n{'=' * 70}")
        print("‚úÖ DEPLOYMENT PACKAGE READY")
        print(f"   üìÅ Location: {output_dir}")
        print(
            f"   üìä Files: {len(list(output_dir.rglob('*')))} | Size: {sum(f.stat().st_size for f in output_dir.rglob('*') if f.is_file()) / 1024:.1f} KB"
        )
        print(f"\nüöÄ To deploy, run:\n   cd {output_dir}\n   ./deploy.sh")
        print(f"{'=' * 70}\n")

        return {
            "status": "ready",
            "blueprint_id": blueprint_id,
            "output_directory": str(output_dir),
            "deployment_log": deployment_log,
            "next_steps": [f"cd {output_dir}", "./deploy.sh"],
        }

    def _generate_deployment_readme(self, blueprint: QuantumBlueprint) -> str:
        return f"""# Deployment Guide: {blueprint.title}

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Overview
{blueprint.description}

## Services
{chr(10).join(f"- {s}" for s in blueprint.architecture.get('services', []))}

## Commands
```bash
{chr(10).join(blueprint.deployment_plan.get('commands', []))}
```
"""

    def _generate_deploy_script(self, blueprint: QuantumBlueprint) -> str:
        return f"""#!/bin/bash
set -e

echo "Starting deployment‚Ä¶"
# Example build
docker build -t quantum-app:latest .
# Example apply
kubectl apply -f kubernetes/
"""

    # ----- Mutation / Autopilot -----
    def mutate_vision(self, seed: Optional[str] = None) -> str:
        base = seed or (self.run_log[-1] if self.run_log else "intelligent web dashboard")
        docs = self.kg.semantic_search(base, limit=5)
        concepts = []
        for d in docs:
            concepts.extend(d.domain_concepts)
        concepts = sorted(set(concepts))[:6]
        vision = f"Build a {base} with {'/'.join(c.lower() for c in concepts)}"
        self.run_log.append(vision)
        return vision

    async def autopilot(self, rounds: int = 3, seed: Optional[str] = None):
        last_blueprint: Optional[QuantumBlueprint] = None
        for r in range(rounds):
            print(f"\n=== AUTOPILOT ROUND {r+1}/{rounds} ===")
            vision = self.mutate_vision(seed if r == 0 else None)
            bp = await self.execute_vision(vision)
            if bp is None:
                print("Stopping autopilot: no blueprint generated.")
                break
            last_blueprint = bp
        return last_blueprint


# ===== Commander (CLI) =====
class QuantumCommanderV4:
    def __init__(self, knowledge_base_path: str = "cpux"):
        self.knowledge_base_path = Path(knowledge_base_path)
        # Always use enhanced KG to avoid missing dependency issues
        self.kg = EnhancedKnowledgeGraph(str(self.knowledge_base_path))
        self.architect = QuantumArchitect(self.kg)
        self.digester = DocumentDigester(self.kg)
        print("\n" + "üî±" * 23)
        print("‚öîÔ∏è  QUANTUM COMMANDER V4.0 INITIALIZED")
        print("   THE ULTIMATE EXECUTIONER")
        print("üî±" * 23)

        # Initial gather from provided knowledge dir + repo docs
        self.gather_initial_knowledge()

    def gather_initial_knowledge(self):
        total = 0
        # Priority: explicit knowledge dir
        total += self.kg.ingest_path_recursive(self.knowledge_base_path, category="knowledge") or 0
        # Repo docs
        total += self.kg.ingest_path_recursive(Path("docs"), category="docs") or 0
        # Whole repository (light)
        total += self.kg.ingest_path_recursive(Path("."), category="repo") or 0
        if total:
            self.kg.build_index()
            self.kg.build_relationships()
            self.kg.calculate_centrality()
            self.kg.save_memory()
        print(f"üóÉÔ∏è  Initial gather done: {total} files")

    async def interactive_session(self):
        print("\n" + "=" * 70)
        print("‚öîÔ∏è  QUANTUM COMMANDER READY")
        print("   Type 'help' for commands")
        print("=" * 70)

        while True:
            try:
                user_input = input("\n‚öîÔ∏è  > ").strip()
                if not user_input:
                    continue
                parts = user_input.split(maxsplit=1)
                command = parts[0].lower()
                args = parts[1] if len(parts) > 1 else ""

                if command == "exit":
                    print("\nüëã Shutting down‚Ä¶")
                    self.kg.save_memory()
                    break

                if command == "help":
                    self._print_help()
                    continue

                if command == "gather":
                    path = Path(args) if args else self.knowledge_base_path
                    count = self.kg.ingest_path_recursive(path, category="gather")
                    self.kg.build_index()
                    self.kg.build_relationships()
                    self.kg.calculate_centrality()
                    self.kg.save_memory()
                    print(f"‚úÖ Gathered {count} files from {path}")
                    continue

                if command == "search":
                    if not args:
                        print("Usage: search <query>")
                        continue
                    results = self.kg.semantic_search(args, limit=10)
                    for i, d in enumerate(results, 1):
                        print(f"\n{i}. {Path(d.file_path).name}")
                        print(f"   üìÇ {d.category} | ‚≠ê {d.quality_score:.2f}")
                        if d.domain_concepts:
                            print(f"   üí° {', '.join(d.domain_concepts[:5])}")
                    continue

                if command == "digest":
                    # digest <path or query>
                    if not args:
                        print("Usage: digest <path|query>")
                        continue
                    p = Path(args)
                    summaries: List[str] = []
                    if p.exists():
                        # File/directory digest
                        if p.is_file():
                            text = p.read_text(encoding="utf-8", errors="ignore")
                            if p.suffix.lower() in DocumentDigester.CODE_EXTS:
                                summaries.append(self.digester.summarize_code(text, p.name))
                            else:
                                summaries.append(self.digester.summarize(text, p.name))
                        else:
                            for fp in p.rglob("*.*"):
                                if fp.is_file() and fp.stat().st_size < 2_000_000:
                                    try:
                                        text = fp.read_text(encoding="utf-8", errors="ignore")
                                    except Exception:
                                        continue
                                    if fp.suffix.lower() in DocumentDigester.CODE_EXTS:
                                        summaries.append(self.digester.summarize_code(text, fp.name))
                                    else:
                                        summaries.append(self.digester.summarize(text, fp.name))
                    else:
                        # Query digest from KG
                        docs = self.kg.semantic_search(args, limit=8)
                        for d in docs:
                            # Heuristic: if looks like code, use code summary
                            looks_code = any(tok in d.name.lower() for tok in [".py", ".ts", ".js", ".go", ".rs", ".java"]) or "def " in d.full_content or "class " in d.full_content
                            if looks_code:
                                summaries.append(self.digester.summarize_code(d.full_content, d.name))
                            else:
                                summaries.append(self.digester.summarize(d.full_content, d.name))

                    digest_blob = "\n\n".join(summaries[:20])
                    self.kg.ingest_text(digest_blob, category="digest", name=f"digest_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
                    self.kg.build_index()
                    self.kg.save_memory()
                    print("‚úÖ Digest created and ingested.")
                    continue

                if command == "execute":
                    if not args:
                        print("Usage: execute <vision>")
                        continue
                    bp = await self.architect.execute_vision(args, "cloud")
                    if bp:
                        print(f"\nüíé Blueprint ID: {bp.id}")
                    continue

                if command == "deploy":
                    if not args:
                        print("Usage: deploy <blueprint_id>")
                        continue
                    result = await self.architect.deploy_blueprint(args, "staging")
                    print(json.dumps(result, indent=2))
                    continue

                if command == "blueprints":
                    if not self.architect.blueprints:
                        print("üìã No blueprints yet.")
                    else:
                        print(f"\nüìã Available Blueprints ({len(self.architect.blueprints)}):\n")
                        for bp in self.architect.blueprints.values():
                            print(f"   üî± {bp.id[:12]}‚Ä¶ - {bp.title}")
                            print(
                                f"      Artifacts: {len(bp.artifacts)} | Cost: ${bp.cost_estimate.get('monthly_total', 0)}/mo"
                            )
                    continue

                if command == "export":
                    if not args:
                        print("Usage: export <blueprint_id>")
                        continue
                    path = self.export_blueprint(args)
                    if path:
                        print(f"‚úÖ Exported to: {path}")
                    else:
                        print("‚ùå Blueprint not found")
                    continue

                if command == "mutate":
                    vision = self.architect.mutate_vision(args or None)
                    print(f"üß¨ New vision: {vision}")
                    continue

                if command == "autopilot":
                    if not args:
                        rounds, seed = 3, None
                    else:
                        parts2 = args.split()
                        rounds = int(parts2[0]) if parts2 else 3
                        seed = parts2[1] if len(parts2) > 1 else None
                    await self.architect.autopilot(rounds=rounds, seed=seed)
                    continue

                if command == "nlp":
                    if not args:
                        print("Usage: nlp <instruction>")
                        continue
                    reply, new_artifacts = self._nlp.handle(args)
                    print(reply)
                    if new_artifacts:
                        # Ingest created artifacts into KG and write them to output dir
                        self.kg.ingest_text("\n\n".join(a.content for a in new_artifacts), category="artifact", name=f"nlp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
                        self.kg.build_index()
                        self.kg.save_memory()
                    continue

                print(f"‚ùå Unknown command: '{command}'. Type 'help' for commands.")

            except KeyboardInterrupt:
                print("\n\nüëã Interrupted. Shutting down‚Ä¶")
                self.kg.save_memory()
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")
                import traceback
                traceback.print_exc()

    def _print_help(self):
        print("\n" + "=" * 70)
        print("‚öîÔ∏è  QUANTUM COMMANDER V4.0 COMMANDS")
        print("=" * 70)
        print("\nüî± EXECUTION:")
        print("  execute <vision>        ‚Üí Generate complete deployable system")
        print("  deploy <blueprint_id>   ‚Üí Package artifacts and deployment files")
        print("  blueprints              ‚Üí List generated blueprints")
        print("  export <blueprint_id>   ‚Üí Export blueprint summary to JSON")
        print("\nüîç KNOWLEDGE:")
        print("  gather [path]           ‚Üí Ingest knowledge recursively and reindex")
        print("  search <query>          ‚Üí Semantic search across knowledge")
        print("  digest <path|query>     ‚Üí Summarize code/docs and ingest digest")
        print("  nlp <instruction>       ‚Üí Natural language interface (locate/understand/describe/create)")
        print("\nüß¨ GROWTH:")
        print("  mutate [seed]           ‚Üí Generate a mutated vision based on knowledge")
        print("  autopilot <n> [seed]    ‚Üí Run n mutation+execute rounds")
        print("\n‚öôÔ∏è  SYSTEM:")
        print("  help                    ‚Üí Show this help")
        print("  exit                    ‚Üí Exit commander")
        print("=" * 70)

    @property
    def _nlp(self) -> NLPEngine:
        # Lazy init to avoid circular ordering issues
        if not hasattr(self, "__nlp"):
            self.__nlp = NLPEngine(self.kg, self.digester, self.architect)
        return self.__nlp


    def export_blueprint(self, blueprint_id: str, fmt: str = "json") -> Optional[str]:
        bp = self.architect.blueprints.get(blueprint_id)
        if not bp:
            return None
        out_dir = Path("blueprints")
        out_dir.mkdir(exist_ok=True)
        if fmt == "json":
            out = out_dir / f"{blueprint_id}.json"
            data = {
                "id": bp.id,
                "title": bp.title,
                "description": bp.description,
                "architecture": bp.architecture,
                "tech_stack": bp.tech_stack,
                "estimated_timeline": bp.estimated_timeline,
                "cost_estimate": bp.cost_estimate,
                "artifacts": [
                    {
                        "name": a.name,
                        "type": a.artifact_type,
                        "language": a.language,
                        "path": a.file_path,
                        "lines": len(a.content.splitlines()),
                    }
                    for a in bp.artifacts
                ],
                "created_at": bp.created_at,
            }
            out.write_text(json.dumps(data, indent=2))
            return str(out)
        return None


# ===== main =====
async def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "knowledge_path",
        nargs="?",
        default="cpux",
        help="Path to the knowledge base directory",
    )
    parser.add_argument(
        "--seed",
        default=None,
        help="Optional seed text to guarantee non-empty KG",
    )
    args = parser.parse_args()

    commander = QuantumCommanderV4(args.knowledge_path)

    # Ensure at least one seed document to avoid empty KG
    if not commander.kg.documents and args.seed:
        commander.kg.ingest_text(args.seed, category="vision", name="seed.txt")
        commander.kg.build_index()
        commander.kg.save_memory()

    await commander.interactive_session()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nüëã Exiting‚Ä¶")
